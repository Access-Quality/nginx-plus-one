name: Deploy AKS + Cine
run-name: "Deploy AKS + Cine (${{ github.ref_name }})"

on:
  workflow_dispatch:

permissions:
  contents: read
  packages: write # needed to push to ghcr.io

env:
  AZURE_LOCATION: ${{ vars.AZURE_LOCATION }}
  TF_CLOUD_ORGANIZATION: ${{ secrets.TFC_ORG }}
  TF_TOKEN_app_terraform_io: ${{ secrets.TFC_TOKEN }}

jobs:
  # ─────────────────────────────────────────────
  # Job 1 – Verify credentials & setup
  # ─────────────────────────────────────────────
  setup:
    name: Verify Credentials & Setup
    runs-on: ubuntu-latest
    outputs:
      subscription_id: ${{ steps.az-identity.outputs.subscription_id }}

    steps:
      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'

      - name: Verify Azure identity & export subscription ID
        id: az-identity
        run: |
          SUB_ID=$(az account show --query id -o tsv)
          TENANT=$(az account show --query tenantId -o tsv)
          ACCOUNT=$(az account show --query user.name -o tsv)
          echo "subscription_id=${SUB_ID}" >> $GITHUB_OUTPUT
          echo "Azure Subscription : ${SUB_ID}"
          echo "Azure Tenant       : ${TENANT}"
          echo "Azure Account      : ${ACCOUNT}"
          echo "Location           : ${{ vars.AZURE_LOCATION }}"

      - name: Verify Terraform Cloud token
        run: |
          STATUS=$(curl -sf \
            --header "Authorization: Bearer ${{ secrets.TFC_TOKEN }}" \
            "https://app.terraform.io/api/v2/account/details" \
            | jq -r '.data.attributes.username')
          echo "TFC user: ${STATUS}"

  # ─────────────────────────────────────────────
  # Job 2 – Build & push Docker image
  # ─────────────────────────────────────────────
  build-image:
    name: Build & Push Docker Image
    runs-on: ubuntu-latest
    needs: setup
    outputs:
      image: ${{ steps.meta.outputs.image }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push image
        id: meta
        run: |
          IMAGE="ghcr.io/${{ github.repository_owner }}/cine:${{ github.sha }}"
          docker buildx build \
            --platform linux/amd64 \
            --push \
            --tag "${IMAGE}" \
            nginx-eks/app/
          echo "image=${IMAGE}" >> $GITHUB_OUTPUT
          echo "Image pushed: ${IMAGE}"

  # ─────────────────────────────────────────────
  # Job 2b – Build & push Docker image (cine-tmdb)
  # ─────────────────────────────────────────────
  build-image-tmdb:
    name: Build & Push Docker Image (cine-tmdb)
    runs-on: ubuntu-latest
    needs: setup
    outputs:
      image: ${{ steps.meta.outputs.image }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push image
        id: meta
        run: |
          IMAGE="ghcr.io/${{ github.repository_owner }}/cine-tmdb:${{ github.sha }}"
          docker buildx build \
            --platform linux/amd64 \
            --push \
            --tag "${IMAGE}" \
            nginx-eks/cine-tmdb/
          echo "image=${IMAGE}" >> $GITHUB_OUTPUT
          echo "Image pushed: ${IMAGE}"

  # ─────────────────────────────────────────────
  # Job 2c – Pull & push NGINX Plus NIC image
  #          (official image already ships nginx-agent)
  # ─────────────────────────────────────────────
  build-nic:
    name: Pull & Push NGINX Plus NIC Image
    runs-on: ubuntu-latest
    needs: setup
    outputs:
      image: ${{ steps.meta.outputs.image }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure Docker mTLS for private-registry.nginx.com
        run: |
          # private-registry.nginx.com uses mTLS client-cert auth.
          # Place cert/key where the Docker daemon expects them.
          sudo mkdir -p /etc/docker/certs.d/private-registry.nginx.com
          printf '%s' "${{ secrets.NGINX_REPO_CRT }}" \
            | sudo tee /etc/docker/certs.d/private-registry.nginx.com/client.cert > /dev/null
          printf '%s' "${{ secrets.NGINX_REPO_KEY }}" \
            | sudo tee /etc/docker/certs.d/private-registry.nginx.com/client.key > /dev/null
          sudo chmod 0400 \
            /etc/docker/certs.d/private-registry.nginx.com/client.cert \
            /etc/docker/certs.d/private-registry.nginx.com/client.key

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull NIC base image via mTLS
        run: |
          DOCKERFILE="aks-nginx-plus/docker/nginx-nic/Dockerfile"
          NIC_VERSION=$(grep '^ARG NIC_VERSION' "${DOCKERFILE}" \
            | head -1 | cut -d= -f2 | tr -d '[:space:]')
          if [[ -z "${NIC_VERSION}" ]]; then
            echo "ERROR: could not parse NIC_VERSION from ${DOCKERFILE}"
            exit 1
          fi
          SRC="private-registry.nginx.com/nginx-ic-nap/nginx-plus-ingress:${NIC_VERSION}"
          echo "==> Pulling base image: ${SRC}"
          docker pull "${SRC}"

      - name: Compile NAP WAF v5 policy bundle
        run: |
          # Uses private-registry.nginx.com/nap/waf-compiler — same mTLS auth.
          # Output: cine-waf.tgz in the Docker build context (will be COPYed
          # into the image by the Dockerfile).
          DOCKERFILE="aks-nginx-plus/docker/nginx-nic/Dockerfile"
          COMPILER_VERSION=$(grep '^ARG COMPILER_VERSION' "${DOCKERFILE}" \
            | head -1 | cut -d= -f2 | tr -d '[:space:]')
          echo "==> WAF compiler version: ${COMPILER_VERSION}"
          # The waf-compiler container runs as a non-root user — the mounted
          # directory must be world-writable so it can write the output bundle.
          chmod o+w aks-nginx-plus/docker/nginx-nic/
          docker run --rm \
            -v "$PWD/aks-nginx-plus/docker/nginx-nic":/workspace \
            "private-registry.nginx.com/nap/waf-compiler:${COMPILER_VERSION}" \
            -p /workspace/waf-policy.json \
            -o /workspace/cine-waf.tgz
          echo "==> Bundle: $(ls -lh aks-nginx-plus/docker/nginx-nic/cine-waf.tgz)"

      - name: Build and push NIC image with WAF bundle
        id: meta
        run: |
          # docker build (not buildx) reuses the already-pulled base image from
          # the local daemon cache — no second mTLS pull needed.
          DOCKERFILE="aks-nginx-plus/docker/nginx-nic/Dockerfile"
          NIC_VERSION=$(grep '^ARG NIC_VERSION' "${DOCKERFILE}" \
            | head -1 | cut -d= -f2 | tr -d '[:space:]')
          IMAGE="ghcr.io/${{ github.repository_owner }}/nginx-nic:${{ github.sha }}"
          echo "==> IMAGE: ${IMAGE}"
          docker build \
            --build-arg NIC_VERSION="${NIC_VERSION}" \
            --tag "${IMAGE}" \
            aks-nginx-plus/docker/nginx-nic/
          docker push "${IMAGE}"
          echo "image=${IMAGE}" >> $GITHUB_OUTPUT
          echo "NIC image pushed: ${IMAGE}"

      - name: Clean up Docker mTLS certs
        if: always()
        run: sudo rm -rf /etc/docker/certs.d/private-registry.nginx.com

  # ─────────────────────────────────────────────
  # Job 3 – Terraform Plan
  # ─────────────────────────────────────────────
  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: setup

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Terraform Init
        working-directory: aks-nginx-plus/terraform/aks
        env:
          ARM_CLIENT_ID:       ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET:   ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_TENANT_ID:       ${{ secrets.AZURE_TENANT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        run: terraform init

      - name: Force-unlock workspace (if locked from a previous failed run)
        run: |
          WORKSPACE_ID=$(curl -sf \
            --header "Authorization: Bearer ${{ secrets.TFC_TOKEN }}" \
            --header "Content-Type: application/vnd.api+json" \
            "https://app.terraform.io/api/v2/organizations/${{ secrets.TFC_ORG }}/workspaces/nginx-aks" \
            | jq -r '.data.id')

          if [[ -z "${WORKSPACE_ID}" || "${WORKSPACE_ID}" == "null" ]]; then
            echo "No se pudo obtener el workspace ID. Continuando sin desbloquear."
            exit 0
          fi

          LOCKED=$(curl -sf \
            --header "Authorization: Bearer ${{ secrets.TFC_TOKEN }}" \
            --header "Content-Type: application/vnd.api+json" \
            "https://app.terraform.io/api/v2/workspaces/${WORKSPACE_ID}" \
            | jq -r '.data.attributes.locked')

          if [[ "${LOCKED}" == "true" ]]; then
            echo "Workspace bloqueado. Ejecutando force-unlock..."
            curl -sf \
              --header "Authorization: Bearer ${{ secrets.TFC_TOKEN }}" \
              --header "Content-Type: application/vnd.api+json" \
              --request POST \
              "https://app.terraform.io/api/v2/workspaces/${WORKSPACE_ID}/actions/force-unlock"
            echo "Workspace desbloqueado."
          else
            echo "Workspace libre. Continuando."
          fi

      - name: Terraform Validate
        working-directory: aks-nginx-plus/terraform/aks
        env:
          ARM_CLIENT_ID:       ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET:   ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_TENANT_ID:       ${{ secrets.AZURE_TENANT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        run: terraform validate

      - name: Import orphaned Azure resources (if any)
        working-directory: aks-nginx-plus/terraform/aks
        env:
          ARM_CLIENT_ID:       ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET:   ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_TENANT_ID:       ${{ secrets.AZURE_TENANT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        run: |
          SUB_ID="${{ secrets.AZURE_SUBSCRIPTION_ID }}"
          RG_NAME="nginx-aks-rg"
          CLUSTER="nginx-aks"

          echo "Intentando importar recursos huérfanos (los errores se ignoran)..."

          terraform import \
            -var="location=${{ vars.AZURE_LOCATION }}" \
            azurerm_resource_group.main \
            "/subscriptions/${SUB_ID}/resourceGroups/${RG_NAME}" \
            2>&1 | grep -v "already managed\|No changes" || true

          terraform import \
            -var="location=${{ vars.AZURE_LOCATION }}" \
            azurerm_log_analytics_workspace.main \
            "/subscriptions/${SUB_ID}/resourceGroups/${RG_NAME}/providers/Microsoft.OperationalInsights/workspaces/nginx-aks-logs" \
            2>&1 | grep -v "already managed\|No changes" || true

          terraform import \
            -var="location=${{ vars.AZURE_LOCATION }}" \
            azurerm_kubernetes_cluster.main \
            "/subscriptions/${SUB_ID}/resourceGroups/${RG_NAME}/providers/Microsoft.ContainerService/managedClusters/${CLUSTER}" \
            2>&1 | grep -v "already managed\|No changes" || true

          echo "Importación completada."

      - name: Terraform Plan
        working-directory: aks-nginx-plus/terraform/aks
        env:
          ARM_CLIENT_ID:       ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET:   ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_TENANT_ID:       ${{ secrets.AZURE_TENANT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        run: |
          terraform plan \
            -var="location=${{ vars.AZURE_LOCATION }}" \
            -out=tfplan

      - name: Upload tfplan artifact
        uses: actions/upload-artifact@v4
        with:
          name: tfplan
          path: aks-nginx-plus/terraform/aks/tfplan
          retention-days: 1

  # ─────────────────────────────────────────────
  # Job 4 – Terraform Apply
  # ─────────────────────────────────────────────
  terraform-apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: terraform-plan
    outputs:
      cluster_name:        ${{ steps.tf-output.outputs.cluster_name }}
      resource_group_name: ${{ steps.tf-output.outputs.resource_group_name }}
      location:            ${{ steps.tf-output.outputs.location }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Terraform Init
        working-directory: aks-nginx-plus/terraform/aks
        env:
          ARM_CLIENT_ID:       ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET:   ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_TENANT_ID:       ${{ secrets.AZURE_TENANT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        run: terraform init

      - name: Download tfplan artifact
        uses: actions/download-artifact@v4
        with:
          name: tfplan
          path: aks-nginx-plus/terraform/aks/

      - name: Terraform Apply
        working-directory: aks-nginx-plus/terraform/aks
        env:
          ARM_CLIENT_ID:       ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET:   ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_TENANT_ID:       ${{ secrets.AZURE_TENANT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        run: terraform apply -auto-approve tfplan

      - name: Capture Terraform outputs
        id: tf-output
        working-directory: aks-nginx-plus/terraform/aks
        env:
          ARM_CLIENT_ID:       ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET:   ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_TENANT_ID:       ${{ secrets.AZURE_TENANT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        run: |
          echo "cluster_name=$(terraform output -raw cluster_name)"               >> $GITHUB_OUTPUT
          echo "resource_group_name=$(terraform output -raw resource_group_name)" >> $GITHUB_OUTPUT
          echo "location=$(terraform output -raw location)"                       >> $GITHUB_OUTPUT

  # ─────────────────────────────────────────────
  # Job 5 – Install NGINX Plus Ingress Controller
  #          (NIC + App Protect WAF + NGINX Agent)
  # ─────────────────────────────────────────────
  setup-ingress:
    name: Install NGINX Plus NIC (NAP WAF + NGINX One Agent)
    runs-on: ubuntu-latest
    needs: [terraform-apply, build-nic]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'

      - name: Get AKS credentials (kubeconfig)
        run: |
          az aks get-credentials \
            --resource-group ${{ needs.terraform-apply.outputs.resource_group_name }} \
            --name           ${{ needs.terraform-apply.outputs.cluster_name }} \
            --overwrite-existing

      - name: Setup Helm
        uses: azure/setup-helm@v4

      - name: Uninstall OSS ingress-nginx (if present)
        run: |
          if helm status ingress-nginx -n ingress-nginx &>/dev/null; then
            echo "Removing OSS ingress-nginx..."
            helm uninstall ingress-nginx -n ingress-nginx --wait
          else
            echo "OSS ingress-nginx not found — skipping uninstall."
          fi

      - name: Create nginx-ingress namespace
        run: |
          kubectl create namespace nginx-ingress --dry-run=client -o yaml \
            | kubectl apply -f -

      - name: Create GHCR imagePullSecret for NIC
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=${{ github.repository_owner }} \
            --docker-password=${{ secrets.GITHUB_TOKEN }} \
            --namespace nginx-ingress \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Create NGINX Plus license secret
        run: |
          # Write values to temp files to safely handle newlines / special chars
          printf '%s' "${{ secrets.LICENSE_JWT }}" > /tmp/license.jwt
          printf '%s' "${{ secrets.LICENSE_KEY }}" > /tmp/license.key
          # Secret type nginx.com/license is immutable — delete first if it exists with wrong type
          kubectl delete secret nginx-license -n nginx-ingress --ignore-not-found
          kubectl create secret generic nginx-license \
            --from-file=license.jwt=/tmp/license.jwt \
            --from-file=license.key=/tmp/license.key \
            --type=nginx.com/license \
            --namespace nginx-ingress
          rm -f /tmp/license.jwt /tmp/license.key

      - name: Create NGINX One data plane key secret
        # Secret key must be 'dataplane.key' — required by the Helm chart
        # nginxAgent.dataplaneKeySecretName value.
        # Ref: https://docs.nginx.com/nginx-one-console/k8s/add-nic/
        run: |
          printf '%s' "${{ secrets.DATA_PLANE_KEY }}" > /tmp/dataplane.key
          kubectl delete secret nginx-one-token -n nginx-ingress --ignore-not-found
          kubectl create secret generic nginx-one-token \
            --from-file=dataplane.key=/tmp/dataplane.key \
            --namespace nginx-ingress
          rm -f /tmp/dataplane.key

      - name: Delete stale NIC LoadBalancer service (force Azure CCM re-provisioning)
        run: |
          # If the service already exists from a previous failed run, the Azure
          # Cloud Controller Manager will NOT retry provisioning the LB automatically.
          # Deleting it here forces a clean creation with the correct configuration.
          kubectl delete svc nginx-ingress-controller -n nginx-ingress --ignore-not-found
          echo "Stale service deleted (if it existed). Helm will recreate it."

      - name: Install NGINX Plus NIC with App Protect WAF
        run: |
          NIC_IMAGE="${{ needs.build-nic.outputs.image }}"
          if [[ -z "${NIC_IMAGE}" ]]; then
            echo "ERROR: build-nic output image is empty"
            exit 1
          fi
          # Use the official OCI chart — it includes nginxAgent.* support
          # Ref: https://docs.nginx.com/nginx-one-console/k8s/add-nic/
          helm upgrade --install nginx-ingress \
            oci://ghcr.io/nginx/charts/nginx-ingress --version 2.4.4 \
            --namespace nginx-ingress \
            --values aks-nginx-plus/k8s/nginx-ic/nic-values.yaml \
            --set controller.image.repository="$(cut -d: -f1 <<< "${NIC_IMAGE}")" \
            --set controller.image.tag="$(cut -d: -f2 <<< "${NIC_IMAGE}")" \
            --wait \
            --timeout 10m

      - name: Diagnose NIC pod (runs on failure)
        if: failure()
        run: |
          echo "=== Pods in nginx-ingress ==="
          kubectl get pods -n nginx-ingress -o wide
          echo ""
          echo "=== Describe NIC pod ==="
          POD=$(kubectl get pod -n nginx-ingress -l app=nginx-ingress -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
          if [[ -n "${POD}" ]]; then
            kubectl describe pod "${POD}" -n nginx-ingress
            echo ""
            echo "=== NIC pod logs (last 100 lines) ==="
            kubectl logs "${POD}" -n nginx-ingress --tail=100 || true
          else
            echo "No NIC pod found"
          fi
          echo ""
          echo "=== Events in nginx-ingress namespace ==="
          kubectl get events -n nginx-ingress --sort-by='.lastTimestamp' | tail -30

      - name: Remove obsolete NAP v4 CRDs (APPolicy/APLogConf not used with NAP v5)
        run: |
          # NAP WAF v5 uses pre-compiled bundles baked into the NIC image.
          # APPolicy + APLogConf CRDs only apply to NAP v4.
          # The policy is activated via a k8s.nginx.org/v1 Policy CRD
          # referencing the bundle, applied per-namespace in the deploy-app job.
          echo "NAP v5 bundle is embedded in the NIC image — no APPolicy CRDs to apply."

      - name: Verify NIC deployment
        run: |
          kubectl rollout status deployment/nginx-ingress-controller \
            -n nginx-ingress --timeout=3m
          kubectl get svc nginx-ingress-controller -n nginx-ingress

  # ─────────────────────────────────────────────
  # Job 6 – Deploy the app
  # ─────────────────────────────────────────────
  deploy-app:
    name: Deploy Cine + Cine-TMDB
    runs-on: ubuntu-latest
    needs:
      [terraform-apply, setup-ingress, build-image, build-image-tmdb, build-nic]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'

      - name: Get AKS credentials (kubeconfig)
        run: |
          az aks get-credentials \
            --resource-group ${{ needs.terraform-apply.outputs.resource_group_name }} \
            --name           ${{ needs.terraform-apply.outputs.cluster_name }} \
            --overwrite-existing

      - name: Create OMDB API key secret
        run: |
          kubectl apply -f nginx-eks/k8s/namespace.yaml
          kubectl create secret generic omdb-secret \
            --from-literal=OMDB_API_KEY=${{ secrets.OMDB_API_KEY }} \
            --namespace cine \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Inject image into app.yaml
        run: |
          IMAGE="${{ needs.build-image.outputs.image }}"
          if [[ -z "${IMAGE}" ]]; then
            echo "ERROR: build-image output is empty"
            exit 1
          fi
          sed -i "s|IMAGE_PLACEHOLDER|${IMAGE}|g" nginx-eks/k8s/app.yaml
          echo "Image injected: ${IMAGE}"
          grep 'image:' nginx-eks/k8s/app.yaml

      - name: Apply Kubernetes manifests
        run: |
          kubectl apply -f nginx-eks/k8s/app.yaml
          # NAP WAF v5 requires VirtualServer (Ingress annotations not supported).
          # Delete any stale Ingress before applying VirtualServer for same host.
          kubectl delete ingress cine -n cine --ignore-not-found
          kubectl apply -f nginx-eks/k8s/cine-vs.yaml
          echo "Manifests applied. Verifying deployment exists..."
          kubectl get deployment cine -n cine

      - name: Wait for Deployment rollout (cine)
        run: |
          echo "Esperando rollout completo del Deployment 'cine'..."
          kubectl rollout status deployment/cine -n cine --timeout=5m

      # ── cine-tmdb ──────────────────────────────────────────────────────────

      - name: Create TMDB API key secret
        run: |
          kubectl apply -f nginx-eks/k8s/cine-tmdb-namespace.yaml
          kubectl create secret generic tmdb-secret \
            --from-literal=TMDB_API_KEY=${{ secrets.TMDB_API_KEY }} \
            --namespace cine-tmdb \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Inject image into cine-tmdb-app.yaml
        run: |
          IMAGE="${{ needs.build-image-tmdb.outputs.image }}"
          if [[ -z "${IMAGE}" ]]; then
            echo "ERROR: build-image-tmdb output is empty"
            exit 1
          fi
          sed -i "s|CINE_TMDB_IMAGE_PLACEHOLDER|${IMAGE}|g" nginx-eks/k8s/cine-tmdb-app.yaml
          echo "Image injected: ${IMAGE}"

      - name: Apply cine-tmdb Kubernetes manifests
        run: |
          kubectl apply -f nginx-eks/k8s/cine-tmdb-app.yaml
          kubectl delete ingress cine-tmdb -n cine-tmdb --ignore-not-found
          kubectl apply -f nginx-eks/k8s/cine-tmdb-vs.yaml
          kubectl get deployment cine-tmdb -n cine-tmdb

      - name: Wait for Deployment rollout (cine-tmdb)
        run: |
          echo "Esperando rollout completo del Deployment 'cine-tmdb'..."
          kubectl rollout status deployment/cine-tmdb -n cine-tmdb --timeout=5m

      - name: Show Ingress LoadBalancer IP
        run: |
          echo "──────────────────────────────────────────────"
          echo "=== NIC Service (current state) ==="
          kubectl get svc nginx-ingress-controller -n nginx-ingress -o wide
          echo ""
          echo "=== Service events (Azure CCM provisioning activity) ==="
          kubectl describe svc nginx-ingress-controller -n nginx-ingress \
            | grep -A 40 '^Events:' || echo "(no events found)"
          echo ""
          echo "Waiting for Azure Load Balancer to assign a public IP..."
          IP=""
          ATTEMPTS=0
          MAX_ATTEMPTS=30   # 30 x 10s = 5 min máximo
          until [[ -n "${IP}" ]] || [[ ${ATTEMPTS} -ge ${MAX_ATTEMPTS} ]]; do
            IP=$(kubectl get svc nginx-ingress-controller \
              -n nginx-ingress \
              -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
            if [[ -z "${IP}" ]]; then
              ATTEMPTS=$((ATTEMPTS + 1))
              echo "  [${ATTEMPTS}/${MAX_ATTEMPTS}] Todavía provisionando... reintentando en 10s"
              sleep 10
            fi
          done

          if [[ -n "${IP}" ]]; then
            echo "NGINX Plus Ingress public IP: ${IP}"
          else
            echo "IP address: no asignada en 5 minutos. Inténtalo manualmente con:"
            echo "  kubectl get svc nginx-ingress-controller -n nginx-ingress"
          fi

          echo ""
          echo "Point  cine.example.com       →  ${IP}"
          echo "Point  cine-tmdb.example.com  →  ${IP}"
          echo "──────────────────────────────────────────────"
